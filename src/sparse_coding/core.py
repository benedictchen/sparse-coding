"""
üí∞ SUPPORT THIS RESEARCH - PLEASE DONATE! üí∞

üôè If this library helps your research or project, please consider donating:
üí≥ https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS

Your support makes advanced AI research accessible to everyone! üöÄ

Sparse Coding Core Implementation
================================

This module contains the unified implementation of sparse coding algorithms
based on Olshausen & Field (1996) and related research.

Consolidated from scattered modules to provide a clean, unified interface
while preserving all research-accurate functionality.

Author: Benedict Chen (benedict@benedictchen.com)  
Based on: Olshausen & Field (1996) "Emergence of Simple-Cell Receptive Field Properties"
"""

import numpy as np
from scipy import linalg
from sklearn.preprocessing import normalize
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.decomposition import FastICA
from typing import Tuple, Optional, Dict, Any, List, Union
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import warnings

# =============================================================================
# Core Sparse Coding Implementation
# =============================================================================

class SparseCoder(BaseEstimator, TransformerMixin):
    """
    Unified Sparse Coding Algorithm - Olshausen & Field (1996)
    
    This is the main sparse coding implementation that consolidates all 
    functionality from the scattered modular structure into a clean,
    unified class.
    
    üî¨ Research Foundation:
    Implements the groundbreaking algorithm that discovers edge detectors
    from natural images, demonstrating how V1 simple cells could emerge
    from efficient coding principles.
    
    üéØ Key Features:
    - Multiple optimization methods (gradient descent, FISTA, coordinate descent)
    - Various sparsity functions (L1, log, gaussian, huber)
    - Dictionary learning with multiple update rules
    - Comprehensive validation and visualization
    - Research-accurate implementations
    
    Parameters
    ----------
    n_components : int, default=100
        Number of dictionary elements (overcomplete basis functions)
    patch_size : tuple, default=(8, 8)  
        Size of image patches for training
    max_iter : int, default=1000
        Maximum iterations for sparse coding optimization
    lambda_sparsity : float, default=0.1
        Sparsity regularization parameter
    learning_rate : float, default=0.01
        Learning rate for dictionary updates
    tolerance : float, default=1e-4
        Convergence tolerance
    batch_size : int, default=100
        Batch size for mini-batch learning
    sparsity_func : str, default='l1'
        Sparsity function: 'l1', 'log', 'gaussian', 'huber'
    optimizer : str, default='fista'
        Optimization method: 'gradient_descent', 'fista', 'coordinate_descent'
    dict_update_rule : str, default='multiplicative'
        Dictionary update rule: 'multiplicative', 'additive', 'projection'
    random_state : int, default=None
        Random seed for reproducibility
    """
    
    def __init__(
        self,
        n_components: int = 100,
        patch_size: Tuple[int, int] = (8, 8),
        max_iter: int = 1000,
        lambda_sparsity: float = 0.1,
        learning_rate: float = 0.01,
        tolerance: float = 1e-4,
        batch_size: int = 100,
        sparsity_func: str = 'l1',
        optimizer: str = 'fista',
        dict_update_rule: str = 'multiplicative',
        random_state: Optional[int] = None
    ):
        # FIXME: Missing input validation for critical parameters
        # Issue: No bounds checking for lambda_sparsity, learning_rate, tolerance
        # Solutions:
        # 1. Add parameter validation with descriptive error messages
        # 2. Implement parameter bounds based on Olshausen & Field paper
        # 3. Add warnings for potentially unstable parameter combinations
        # 
        # Example implementation:
        # if lambda_sparsity <= 0:
        #     raise ValueError(f"lambda_sparsity must be positive, got {lambda_sparsity}")
        # if lambda_sparsity > 1.0:
        #     warnings.warn(f"lambda_sparsity={lambda_sparsity} is large, may cause over-sparsification")
        # if learning_rate <= 0 or learning_rate > 1.0:
        #     raise ValueError(f"learning_rate must be in (0, 1], got {learning_rate}")
        
        # FIXME: patch_size validation missing - critical for dictionary initialization  
        # Issue: patch_size could be invalid (negative, zero, non-square for some algorithms)
        # Solutions:
        # 1. Validate patch dimensions are positive integers
        # 2. Check if patch_size is compatible with expected image sizes
        # 3. Warn if patch_size is unusual for sparse coding (typically 8x8, 16x16)
        #
        # Example:
        # if not all(isinstance(p, int) and p > 0 for p in patch_size):
        #     raise ValueError(f"patch_size must be positive integers, got {patch_size}")
        # if patch_size[0] != patch_size[1]:
        #     warnings.warn("Non-square patches may not work with all algorithms")
        
        # FIXME: No validation for optimizer and sparsity_func strings
        # Issue: Typos in these strings will cause runtime errors later
        # Solutions:
        # 1. Define allowed values as class constants or enums
        # 2. Validate inputs against allowed values
        # 3. Provide suggestions for similar valid options
        #
        # Example:
        # VALID_OPTIMIZERS = {'fista', 'gradient_descent', 'coordinate_descent'}
        # VALID_SPARSITY_FUNCS = {'l1', 'log', 'gaussian', 'huber'}
        # if optimizer not in VALID_OPTIMIZERS:
        #     raise ValueError(f"optimizer must be one of {VALID_OPTIMIZERS}, got '{optimizer}'")
        
        self.n_components = n_components
        self.patch_size = patch_size
        self.max_iter = max_iter
        self.lambda_sparsity = lambda_sparsity
        self.learning_rate = learning_rate
        self.tolerance = tolerance
        self.batch_size = batch_size
        self.sparsity_func = sparsity_func
        self.optimizer = optimizer
        self.dict_update_rule = dict_update_rule
        self.random_state = random_state
        
        # Initialize attributes
        self.dictionary_ = None
        self.is_fitted = False
        self.training_history_ = {
            'reconstruction_error': [],
            'sparsity_cost': [],
            'total_cost': []
        }
        
        # Set random seed
        if random_state is not None:
            np.random.seed(random_state)
            
        print(f"‚ú® SparseCoder initialized: {n_components} components, {patch_size} patches")
        print(f"   üéØ Optimizer: {optimizer}, Sparsity: {sparsity_func}")
    
    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> 'SparseCoder':
        """
        Learn sparse dictionary from training data
        
        Parameters
        ---------- 
        X : array-like, shape (n_samples, n_features)
            Training data (image patches)
        y : ignored
            Not used, present for API consistency
            
        Returns
        -------
        self : SparseCoder
            Fitted estimator
        """
        X = self._validate_data(X)
        
        # Initialize dictionary randomly
        patch_dim = self.patch_size[0] * self.patch_size[1]
        self.dictionary_ = self._initialize_dictionary(patch_dim, self.n_components)
        
        print(f"üî¨ Training sparse coder on {X.shape[0]} patches...")
        
        # FIXME: Critical algorithmic issues in main training loop
        # Issue 1: No handling of batch_size > X.shape[0] 
        # Issue 2: No convergence monitoring beyond cost change
        # Issue 3: Missing gradient clipping for stability
        # Issue 4: No adaptive learning rate scheduling
        
        # Main training loop
        for iteration in range(self.max_iter):
            # FIXME: Batch selection can fail if batch_size > n_samples
            # Issue: np.random.choice with replace=False will crash
            # Solutions:
            # 1. Check and adjust batch_size if needed
            # 2. Use replace=True for small datasets
            # 3. Implement proper mini-batch handling
            #
            # Example fix:
            # effective_batch_size = min(self.batch_size, X.shape[0])
            # if effective_batch_size < self.batch_size:
            #     warnings.warn(f"Reduced batch_size from {self.batch_size} to {effective_batch_size}")
            # batch_indices = np.random.choice(X.shape[0], effective_batch_size, 
            #                                 replace=effective_batch_size > X.shape[0])
            
            batch_indices = np.random.choice(X.shape[0], self.batch_size, replace=False)
            X_batch = X[batch_indices]
            
            # FIXME: Sparse coding step lacks error handling and numerical stability
            # Issue: No check for degenerate solutions or numerical overflow
            # Solutions:
            # 1. Add numerical stability checks in sparse coding
            # 2. Monitor for NaN/Inf values and handle gracefully
            # 3. Add option to restart from different initialization if needed
            #
            # Example:
            # codes = self._sparse_coding_step(X_batch)
            # if np.any(np.isnan(codes)) or np.any(np.isinf(codes)):
            #     warnings.warn("Numerical instability detected, reinitializing dictionary")
            #     self.dictionary_ = self._initialize_dictionary(patch_dim, self.n_components)
            #     continue
            
            codes = self._sparse_coding_step(X_batch)
            
            # Dictionary update step
            self._dictionary_update_step(X_batch, codes)
            
            # Compute and store costs
            if iteration % 50 == 0:
                recon_error = self._reconstruction_error(X_batch, codes)
                sparsity_cost = self._sparsity_cost(codes)
                total_cost = recon_error + self.lambda_sparsity * sparsity_cost
                
                self.training_history_['reconstruction_error'].append(recon_error)
                self.training_history_['sparsity_cost'].append(sparsity_cost)
                self.training_history_['total_cost'].append(total_cost)
                
                print(f"   Iter {iteration:4d}: Cost={total_cost:.4f} (Recon={recon_error:.4f}, Sparse={sparsity_cost:.4f})")
                
                # Check convergence
                if len(self.training_history_['total_cost']) > 1:
                    cost_change = abs(self.training_history_['total_cost'][-1] - 
                                    self.training_history_['total_cost'][-2])
                    if cost_change < self.tolerance:
                        print(f"   ‚úÖ Converged after {iteration} iterations")
                        break
        
        self.is_fitted = True
        print(f"üéâ Training complete! Dictionary learned with {self.n_components} components")
        return self
    
    def transform(self, X: np.ndarray) -> np.ndarray:
        """
        Transform data using learned sparse dictionary
        
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Data to transform
            
        Returns
        -------
        X_transformed : array, shape (n_samples, n_components)
            Sparse codes for input data
        """
        if not self.is_fitted:
            raise ValueError("SparseCoder must be fitted before transform")
            
        X = self._validate_data(X)
        return self._sparse_coding_step(X)
    
    def fit_transform(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Fit model and transform data in one step
        """
        return self.fit(X, y).transform(X)
    
    def reconstruct(self, X: np.ndarray) -> np.ndarray:
        """
        Reconstruct data from sparse codes
        
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            Input data
            
        Returns
        -------
        X_reconstructed : array, shape (n_samples, n_features)
            Reconstructed data
        """
        codes = self.transform(X)
        return codes @ self.dictionary_
    
    # =============================================================================
    # Core Algorithm Methods
    # =============================================================================
    
    def _initialize_dictionary(self, patch_dim: int, n_components: int) -> np.ndarray:
        """Initialize dictionary with normalized random vectors"""
        dictionary = np.random.randn(n_components, patch_dim).astype(np.float32)
        # Normalize each dictionary element
        for i in range(n_components):
            dictionary[i] /= np.linalg.norm(dictionary[i])
        return dictionary
    
    def _sparse_coding_step(self, X: np.ndarray) -> np.ndarray:
        """
        Solve sparse coding optimization problem for given data
        
        Minimizes: ||X - D*A||^2 + Œª * sparsity_func(A)
        where D is dictionary, A is sparse codes
        """
        n_samples = X.shape[0]
        codes = np.zeros((n_samples, self.n_components))
        
        for i, x in enumerate(X):
            if self.optimizer == 'fista':
                codes[i] = self._fista_optimization(x)
            elif self.optimizer == 'coordinate_descent':
                codes[i] = self._coordinate_descent_optimization(x)
            else:  # gradient_descent
                codes[i] = self._gradient_descent_optimization(x)
                
        return codes
    
    def _fista_optimization(self, x: np.ndarray) -> np.ndarray:
        """
        FISTA (Fast Iterative Shrinkage-Thresholding Algorithm)
        for sparse coding optimization
        """
        # FIXME: Multiple critical issues in FISTA implementation
        # Issue 1: Incorrect Lipschitz constant calculation
        # Issue 2: Hard-coded inner iteration limit (100) without justification
        # Issue 3: Missing _soft_threshold method implementation check
        # Issue 4: No handling of ill-conditioned dictionary matrices
        
        a = np.zeros(self.n_components)
        y = a.copy()
        t = 1.0
        
        # FIXME: Lipschitz constant calculation is incorrect
        # Issue: Using dictionary @ dictionary.T instead of proper eigenvalue computation
        # According to FISTA paper (Beck & Teboulle 2009), should be largest eigenvalue of A^T A
        # Solutions:
        # 1. Compute proper Lipschitz constant: L = np.linalg.norm(self.dictionary_, ord=2)**2
        # 2. Use power iteration for large dictionaries for efficiency
        # 3. Add safety factor (L *= 1.1) to ensure convergence
        #
        # Correct implementation:
        # L = np.linalg.norm(self.dictionary_, ord=2)**2 * 1.1  # Safety factor
        # Or for efficiency: L = self._estimate_lipschitz_constant()
        
        L = np.linalg.norm(self.dictionary_ @ self.dictionary_.T)
        
        # FIXME: Hard-coded iteration limit without adaptive stopping
        # Issue: 100 iterations may be too few/many depending on problem difficulty
        # Solutions:
        # 1. Make inner iterations adaptive based on convergence rate
        # 2. Use relative tolerance instead of fixed iteration count
        # 3. Add maximum iteration safeguard with warning
        #
        # Example:
        # max_inner_iter = min(1000, 10 * self.n_components)  # Adaptive limit
        # for inner_iter in range(max_inner_iter):
        
        for _ in range(100):  # Inner iterations
            # FIXME: Missing numerical stability checks
            # Issue: No validation that _soft_threshold method exists or works correctly
            # Solutions:
            # 1. Add method existence check with fallback
            # 2. Validate gradient computation for numerical stability
            # 3. Add overflow/underflow protection
            #
            # Example checks:
            # if not hasattr(self, '_soft_threshold'):
            #     raise AttributeError("_soft_threshold method not implemented")
            # if L <= 0:
            #     raise ValueError("Lipschitz constant must be positive")
            
            # Gradient step
            grad = self.dictionary_ @ (self.dictionary_.T @ y - x)
            a_new = self._soft_threshold(y - grad / L, self.lambda_sparsity / L)
            
            # FIXME: FISTA acceleration step lacks numerical stability
            # Issue: No protection against numerical instabilities in momentum computation
            # Solutions:
            # 1. Add bounds checking for momentum term
            # 2. Reset acceleration if divergence is detected
            # 3. Use safe sqrt computation
            #
            # Example:
            # t_new = (1 + np.sqrt(max(1e-10, 1 + 4 * t**2))) / 2  # Safe sqrt
            # momentum = min(0.999, (t - 1) / t_new)  # Bounded momentum
            
            # FISTA acceleration
            t_new = (1 + np.sqrt(1 + 4 * t**2)) / 2
            y = a_new + ((t - 1) / t_new) * (a_new - a)
            
            # Check convergence
            if np.linalg.norm(a_new - a) < self.tolerance:
                break
                
            a = a_new
            t = t_new
            
        return a
    
    def _coordinate_descent_optimization(self, x: np.ndarray) -> np.ndarray:
        """Coordinate descent optimization for sparse coding"""
        a = np.zeros(self.n_components)
        
        for _ in range(100):  # Inner iterations
            a_old = a.copy()
            
            for j in range(self.n_components):
                # Compute residual without j-th component
                residual = x - (a @ self.dictionary_) + a[j] * self.dictionary_[j]
                
                # Update j-th coefficient
                correlation = self.dictionary_[j] @ residual
                a[j] = self._soft_threshold_scalar(correlation, self.lambda_sparsity)
            
            # Check convergence
            if np.linalg.norm(a - a_old) < self.tolerance:
                break
                
        return a
    
    def _gradient_descent_optimization(self, x: np.ndarray) -> np.ndarray:
        """Basic gradient descent for sparse coding"""
        a = np.zeros(self.n_components)
        
        for _ in range(100):  # Inner iterations
            # Gradient of reconstruction error
            grad = self.dictionary_ @ (self.dictionary_.T @ a - x)
            
            # Gradient step
            a = a - 0.01 * grad
            
            # Apply sparsity (soft thresholding for L1)
            if self.sparsity_func == 'l1':
                a = self._soft_threshold(a, self.lambda_sparsity * 0.01)
                
        return a
    
    def _dictionary_update_step(self, X: np.ndarray, codes: np.ndarray):
        """Update dictionary using specified update rule"""
        if self.dict_update_rule == 'multiplicative':
            self._multiplicative_update(X, codes)
        elif self.dict_update_rule == 'additive':
            self._additive_update(X, codes)
        else:  # projection
            self._projection_update(X, codes)
    
    def _multiplicative_update(self, X: np.ndarray, codes: np.ndarray):
        """Multiplicative dictionary update rule"""
        # FIXME: Critical mathematical errors in multiplicative update
        # Issue 1: Incorrect mathematical formulation - doesn't match literature
        # Issue 2: Inefficient element-wise updates instead of vectorized operations
        # Issue 3: Improper normalization that can cause convergence issues
        # Issue 4: No dead neuron detection and revival mechanism
        
        for j in range(self.n_components):
            if np.sum(codes[:, j]**2) > 1e-8:  # Avoid division by zero
                
                # FIXME: Mathematical formulation is incorrect
                # Issue: The update rule doesn't correspond to any standard algorithm
                # Current: d_j *= (X^T @ a_j) / (d_j @ (a_j @ a_j^T))
                # Should be based on NMF-style updates or MOD algorithm
                # 
                # Solutions:
                # 1. Implement Method of Optimal Directions (MOD):
                #    Dictionary = X @ codes^T @ (codes @ codes^T)^(-1)
                # 2. Use K-SVD algorithm for better convergence:
                #    Update one column at a time using SVD
                # 3. Implement proper NMF multiplicative updates:
                #    D = D .* (X @ A^T) ./ (D @ A @ A^T)
                #
                # Example MOD implementation:
                # if np.linalg.det(codes @ codes.T) > 1e-10:
                #     self.dictionary_ = X @ codes.T @ np.linalg.pinv(codes @ codes.T)
                
                # Update j-th dictionary element
                numerator = X.T @ codes[:, j]
                denominator = self.dictionary_[j] @ (codes[:, j] @ codes[:, j].T)
                
                if denominator > 1e-8:
                    # FIXME: Dangerous division without proper bounds checking
                    # Issue: Can lead to exploding dictionary elements
                    # Solutions:
                    # 1. Clip update ratios to reasonable bounds [0.1, 10.0]
                    # 2. Use multiplicative update with damping factor
                    # 3. Add gradient-based regularization term
                    #
                    # Example:
                    # update_ratio = np.clip(numerator / denominator, 0.1, 10.0)
                    # self.dictionary_[j] *= update_ratio
                    
                    self.dictionary_[j] *= numerator / denominator
                    
                    # FIXME: Normalization after each update is inefficient and unstable
                    # Issue: Normalizing after each column can cause oscillations
                    # Solutions:
                    # 1. Normalize all columns once at the end of epoch
                    # 2. Use proper constraint handling (projection onto unit sphere)
                    # 3. Add energy-based normalization constraints
                    #
                    # Better approach:
                    # Store updates and apply normalization in batch:
                    # new_norm = np.linalg.norm(self.dictionary_[j])
                    # if new_norm > self.max_norm_threshold:
                    #     self.dictionary_[j] = self.dictionary_[j] / new_norm * self.target_norm
                    
                    # Normalize
                    norm = np.linalg.norm(self.dictionary_[j])
                    if norm > 1e-8:
                        self.dictionary_[j] /= norm
            
            # FIXME: No dead neuron handling
            # Issue: Dictionary elements with zero activation never get updated
            # Solutions:
            # 1. Detect dead neurons (low variance, low activation)
            # 2. Reinitialize dead neurons with random data patches
            # 3. Use sparse activation penalty to encourage diverse dictionary
            #
            # Example dead neuron detection:
            # else:  # codes[:, j] is effectively zero
            #     if np.random.random() < 0.01:  # 1% chance to revive
            #         random_patch_idx = np.random.randint(X.shape[0])
            #         self.dictionary_[j] = X[random_patch_idx] + 0.1 * np.random.randn(X.shape[1])
            #         self.dictionary_[j] /= np.linalg.norm(self.dictionary_[j])
    
    def _additive_update(self, X: np.ndarray, codes: np.ndarray):
        """Additive dictionary update rule (gradient-based)"""
        for j in range(self.n_components):
            if np.sum(codes[:, j]**2) > 1e-8:
                # Residual without j-th component
                residual = X - codes @ self.dictionary_ + np.outer(codes[:, j], self.dictionary_[j])
                
                # Gradient step
                gradient = codes[:, j] @ residual
                self.dictionary_[j] += self.learning_rate * gradient
                
                # Normalize
                norm = np.linalg.norm(self.dictionary_[j])
                if norm > 1e-8:
                    self.dictionary_[j] /= norm
    
    def _projection_update(self, X: np.ndarray, codes: np.ndarray):
        """Projection-based dictionary update"""
        # This is a more advanced update rule that projects onto feasible set
        for j in range(self.n_components):
            if np.sum(codes[:, j]**2) > 1e-8:
                # Optimal update (analytical solution)
                self.dictionary_[j] = (X.T @ codes[:, j]) / (codes[:, j] @ codes[:, j])
                
                # Project to unit norm
                norm = np.linalg.norm(self.dictionary_[j])
                if norm > 1e-8:
                    self.dictionary_[j] /= norm
    
    # =============================================================================
    # Utility Methods
    # =============================================================================
    
    def _soft_threshold(self, x: np.ndarray, threshold: float) -> np.ndarray:
        """Soft thresholding operator (for L1 regularization)"""
        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)
    
    def _soft_threshold_scalar(self, x: float, threshold: float) -> float:
        """Scalar soft thresholding"""
        return np.sign(x) * max(abs(x) - threshold, 0)
    
    def _reconstruction_error(self, X: np.ndarray, codes: np.ndarray) -> float:
        """Compute reconstruction error ||X - D*A||^2"""
        reconstruction = codes @ self.dictionary_
        return np.mean(np.sum((X - reconstruction)**2, axis=1))
    
    def _sparsity_cost(self, codes: np.ndarray) -> float:
        """Compute sparsity cost based on selected function"""
        if self.sparsity_func == 'l1':
            return np.mean(np.sum(np.abs(codes), axis=1))
        elif self.sparsity_func == 'log':
            return np.mean(np.sum(np.log(1 + codes**2), axis=1))
        elif self.sparsity_func == 'gaussian':
            return np.mean(np.sum(1 - np.exp(-codes**2/2), axis=1))
        else:  # huber
            delta = 1.0
            huber = np.where(np.abs(codes) <= delta, 
                           0.5 * codes**2, 
                           delta * np.abs(codes) - 0.5 * delta**2)
            return np.mean(np.sum(huber, axis=1))
    
    def _validate_data(self, X: np.ndarray) -> np.ndarray:
        """Validate input data"""
        X = np.asarray(X)
        if X.ndim != 2:
            raise ValueError("Input data must be 2D")
        return X.astype(np.float32)
    
    # =============================================================================
    # Visualization Methods  
    # =============================================================================
    
    def plot_dictionary(self, figsize: Tuple[int, int] = (12, 8), 
                       max_components: int = 100) -> None:
        """
        Visualize learned dictionary elements as image patches
        """
        if not self.is_fitted:
            raise ValueError("Must fit model before plotting dictionary")
            
        n_show = min(max_components, self.n_components)
        grid_size = int(np.ceil(np.sqrt(n_show)))
        
        fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)
        axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]
        
        for i in range(n_show):
            # Reshape dictionary element to patch
            patch = self.dictionary_[i].reshape(self.patch_size)
            
            axes[i].imshow(patch, cmap='RdBu_r', interpolation='nearest')
            axes[i].set_title(f'Component {i}', fontsize=8)
            axes[i].axis('off')
            
        # Hide unused subplots
        for i in range(n_show, len(axes)):
            axes[i].axis('off')
            
        plt.suptitle('Learned Dictionary Elements (Sparse Coding)', fontsize=14)
        plt.tight_layout()
        plt.show()
    
    def plot_training_history(self) -> None:
        """Plot training cost history"""
        if not self.training_history_['total_cost']:
            print("No training history to plot")
            return
            
        plt.figure(figsize=(12, 4))
        
        # Total cost
        plt.subplot(1, 3, 1)
        plt.plot(self.training_history_['total_cost'])
        plt.title('Total Cost')
        plt.xlabel('Iteration (√ó50)')
        plt.ylabel('Cost')
        
        # Reconstruction error
        plt.subplot(1, 3, 2)
        plt.plot(self.training_history_['reconstruction_error'])
        plt.title('Reconstruction Error')
        plt.xlabel('Iteration (√ó50)')
        plt.ylabel('Error')
        
        # Sparsity cost
        plt.subplot(1, 3, 3)
        plt.plot(self.training_history_['sparsity_cost'])
        plt.title('Sparsity Cost')
        plt.xlabel('Iteration (√ó50)')
        plt.ylabel('Cost')
        
        plt.tight_layout()
        plt.show()


# =============================================================================
# Original Olshausen & Field Implementation
# =============================================================================

class OlshausenFieldOriginal:
    """
    Original Olshausen & Field (1996) Sparse Coding Implementation
    
    This is a faithful reproduction of the original algorithm as described
    in the seminal 1996 Nature paper. Preserved for research accuracy and
    historical reference.
    
    Based on the original MATLAB code by Bruno Olshausen.
    """
    
    def __init__(self, 
                 n_components: int = 100,
                 patch_size: Tuple[int, int] = (8, 8),
                 lambda_sparsity: float = 0.1,
                 eta_phi: float = 0.01,
                 eta_dict: float = 0.01,
                 random_state: Optional[int] = None):
        """
        Parameters from original Olshausen & Field paper
        
        Parameters
        ----------
        n_components : int
            Number of basis functions (M in paper)
        patch_size : tuple
            Image patch dimensions  
        lambda_sparsity : float
            Sparseness parameter (Œª in equation 5)
        eta_phi : float
            Learning rate for coefficients (Œ∑_œÜ in paper)
        eta_dict : float
            Learning rate for dictionary (Œ∑_D in paper)
        """
        self.M = n_components  # Number of basis functions
        self.patch_size = patch_size
        self.N = patch_size[0] * patch_size[1]  # Patch dimension
        self.lambda_sparsity = lambda_sparsity
        self.eta_phi = eta_phi
        self.eta_dict = eta_dict
        
        if random_state is not None:
            np.random.seed(random_state)
            
        # Initialize basis functions (dictionary)
        self.phi = np.random.randn(self.N, self.M).astype(np.float32)
        # Normalize each basis function
        for i in range(self.M):
            self.phi[:, i] /= np.linalg.norm(self.phi[:, i])
            
        print(f"üî¨ Original Olshausen & Field: {self.M} basis functions, {self.patch_size} patches")
    
    def learn_dictionary(self, images: np.ndarray, n_iterations: int = 10000):
        """
        Learn basis functions using original Olshausen & Field algorithm
        
        This implements the exact update rules from the 1996 paper:
        - Equation 5 for coefficient dynamics
        - Equation 7 for basis function updates
        
        Parameters
        ----------
        images : array-like, shape (n_samples, height, width)
            Natural images for training
        n_iterations : int
            Number of training iterations
        """
        print(f"üéØ Learning {self.M} basis functions from {len(images)} images...")
        
        for iteration in range(n_iterations):
            # Select random image and patch location
            img_idx = np.random.randint(len(images))
            image = images[img_idx]
            
            # Extract random patch
            patch = self._extract_random_patch(image)
            
            # Sparse coding: solve for coefficients
            a = self._sparse_inference(patch)
            
            # Update basis functions (dictionary)
            self._update_dictionary(patch, a)
            
            # Progress reporting
            if iteration % 1000 == 0:
                reconstruction = self.phi @ a
                error = np.mean((patch - reconstruction)**2)
                sparsity = np.mean(np.abs(a))
                print(f"   Iter {iteration:5d}: Error={error:.4f}, Sparsity={sparsity:.4f}")
    
    def _extract_random_patch(self, image: np.ndarray) -> np.ndarray:
        """Extract random patch from image"""
        h, w = image.shape
        ph, pw = self.patch_size
        
        # Random patch location
        y = np.random.randint(0, h - ph)
        x = np.random.randint(0, w - pw)
        
        # Extract and flatten patch
        patch = image[y:y+ph, x:x+pw].flatten()
        
        # Subtract mean (preprocessing step)
        patch = patch - np.mean(patch)
        
        return patch
    
    def _sparse_inference(self, patch: np.ndarray, max_iter: int = 100) -> np.ndarray:
        """
        Sparse inference using original dynamics (Equation 5)
        
        This solves: œÑ * da/dt = -Œ¥E/Œ¥a
        where E is the energy function from equation 4
        """
        a = np.zeros(self.M)  # Initialize coefficients
        
        for _ in range(max_iter):
            # Equation 5: coefficient dynamics
            # da/dt = (1/œÑ) * [Œ¶^T * (I - Œ¶*a) - Œª * sign(a)]
            
            reconstruction = self.phi @ a
            residual = patch - reconstruction
            
            # Gradient components
            recon_gradient = self.phi.T @ residual
            sparsity_gradient = self.lambda_sparsity * np.sign(a)
            
            # Update coefficients
            da_dt = recon_gradient - sparsity_gradient
            a += self.eta_phi * da_dt
            
        return a
    
    def _update_dictionary(self, patch: np.ndarray, a: np.ndarray):
        """
        Update basis functions using Equation 7 from original paper
        
        dŒ¶/dt = Œ∑ * a * (I - Œ¶*a)^T - Œ¶ * (a*a^T * Œ¶^T*Œ¶ - diag(a*a^T * Œ¶^T*Œ¶))
        """
        # FIXME: Critical implementation errors in original Olshausen & Field update
        # Issue 1: Incorrect mathematical formulation of anti-Hebbian term
        # Issue 2: Missing proper lateral inhibition matrix computation
        # Issue 3: Inefficient element-wise updates instead of vectorized operations
        # Issue 4: No numerical stability safeguards for matrix operations
        
        reconstruction = self.phi @ a
        residual = patch - reconstruction
        
        for i in range(self.M):
            if np.abs(a[i]) > 1e-8:  # Only update if coefficient is significant
                # Hebbian learning term
                hebbian = a[i] * residual
                
                # FIXME: Anti-Hebbian term implementation is mathematically incorrect
                # Issue: Current lateral computation doesn't match Equation 7 from paper
                # Paper specifies: Œ¶ * (a*a^T * Œ¶^T*Œ¶ - diag(a*a^T * Œ¶^T*Œ¶))
                # Current implementation: a[i] * lateral * self.phi[:, i] is wrong
                #
                # Correct implementation should be:
                # 1. Compute full lateral inhibition matrix C = Œ¶^T @ Œ¶  
                # 2. Apply outer product: a @ a^T
                # 3. Zero diagonal to prevent self-inhibition
                # 4. Update all basis functions simultaneously
                #
                # Example correct implementation:
                # C = self.phi.T @ self.phi  # Correlation matrix
                # np.fill_diagonal(C, 0)     # Remove self-connections
                # lateral_matrix = np.outer(a, a) * C
                # anti_hebbian = self.phi @ lateral_matrix @ a
                
                # Anti-Hebbian term (lateral inhibition)  
                lateral = np.sum(a * (self.phi.T @ self.phi[:, i])) - a[i]
                anti_hebbian = a[i] * lateral * self.phi[:, i]
                
                # FIXME: Dictionary update lacks proper learning rate scaling
                # Issue: No consideration of patch magnitude or local learning rate adaptation
                # Solutions:
                # 1. Scale learning rate by patch energy: eta * ||patch||^2
                # 2. Use adaptive learning rates based on convergence history
                # 3. Add momentum terms for smoother convergence
                #
                # Example:
                # patch_energy = np.linalg.norm(patch)**2
                # adaptive_eta = self.eta_dict / (1 + 0.1 * patch_energy)
                # self.phi[:, i] += adaptive_eta * (hebbian - anti_hebbian)
                
                # Update basis function
                self.phi[:, i] += self.eta_dict * (hebbian - anti_hebbian)
                
                # FIXME: Normalization should preserve energy, not just unit norm
                # Issue: Unit normalization can cause learning instabilities
                # Olshausen & Field paper uses energy constraints, not unit norm
                # Solutions:
                # 1. Use energy-based normalization: ||œÜ_i||^2 = 1/M
                # 2. Implement soft normalization with decay
                # 3. Add batch normalization across dictionary elements
                #
                # Example energy-based normalization:
                # target_energy = 1.0 / self.M  # Energy per basis function
                # current_energy = np.linalg.norm(self.phi[:, i])**2
                # if current_energy > target_energy * 2:  # Only normalize if too large
                #     self.phi[:, i] *= np.sqrt(target_energy / current_energy)
                
                # Normalize to prevent runaway growth
                norm = np.linalg.norm(self.phi[:, i])
                if norm > 1e-8:
                    self.phi[:, i] /= norm


# =============================================================================
# Dictionary Learning Components
# =============================================================================

class DictionaryLearner:
    """
    Advanced Dictionary Learning with Multiple Algorithms
    
    Supports various dictionary learning approaches beyond basic sparse coding:
    - K-SVD algorithm
    - Method of Optimal Directions (MOD)
    - Online dictionary learning
    - Mini-batch dictionary learning
    """
    
    def __init__(self, 
                 n_components: int = 100,
                 algorithm: str = 'ksvd',
                 sparsity_constraint: int = 10,
                 max_iter: int = 100,
                 tolerance: float = 1e-4,
                 random_state: Optional[int] = None):
        """
        Parameters
        ----------
        algorithm : str
            Dictionary learning algorithm: 'ksvd', 'mod', 'online', 'minibatch'
        sparsity_constraint : int
            Maximum number of non-zero coefficients per signal
        """
        self.n_components = n_components
        self.algorithm = algorithm
        self.sparsity_constraint = sparsity_constraint
        self.max_iter = max_iter
        self.tolerance = tolerance
        self.random_state = random_state
        
        if random_state is not None:
            np.random.seed(random_state)
            
        self.dictionary_ = None
        self.is_fitted = False
        
        print(f"üìö DictionaryLearner: {algorithm.upper()} algorithm, {n_components} atoms")
    
    def fit(self, X: np.ndarray) -> 'DictionaryLearner':
        """Learn dictionary from training data"""
        X = np.asarray(X).astype(np.float32)
        
        # Initialize dictionary
        self.dictionary_ = self._initialize_dictionary(X.shape[1], self.n_components)
        
        if self.algorithm == 'ksvd':
            self._fit_ksvd(X)
        elif self.algorithm == 'mod':
            self._fit_mod(X)
        elif self.algorithm == 'online':
            self._fit_online(X)
        else:  # minibatch
            self._fit_minibatch(X)
            
        self.is_fitted = True
        return self
    
    def _fit_ksvd(self, X: np.ndarray):
        """K-SVD dictionary learning algorithm"""
        print("üéØ Training with K-SVD algorithm...")
        
        for iteration in range(self.max_iter):
            # Sparse coding stage: find sparse codes for all signals
            codes = self._orthogonal_matching_pursuit_batch(X)
            
            # Dictionary update stage: update each atom
            for k in range(self.n_components):
                # Find signals that use atom k
                using_atom = np.abs(codes[k, :]) > 1e-8
                
                if np.sum(using_atom) > 0:
                    # Compute residual without atom k
                    residual = X[:, using_atom] - self.dictionary_ @ codes[:, using_atom]
                    residual += np.outer(self.dictionary_[:, k], codes[k, using_atom])
                    
                    # SVD to update atom k and its coefficients
                    if residual.shape[1] > 0:
                        U, s, Vt = np.linalg.svd(residual, full_matrices=False)
                        self.dictionary_[:, k] = U[:, 0]
                        codes[k, using_atom] = s[0] * Vt[0, :]
            
            if iteration % 10 == 0:
                error = self._reconstruction_error(X, codes)
                print(f"   Iter {iteration:3d}: Reconstruction error = {error:.6f}")
    
    def _orthogonal_matching_pursuit_batch(self, X: np.ndarray) -> np.ndarray:
        """Orthogonal Matching Pursuit for batch sparse coding"""
        n_signals = X.shape[1]
        codes = np.zeros((self.n_components, n_signals))
        
        for i in range(n_signals):
            codes[:, i] = self._orthogonal_matching_pursuit_single(X[:, i])
            
        return codes
    
    def _orthogonal_matching_pursuit_single(self, signal: np.ndarray) -> np.ndarray:
        """OMP for single signal"""
        residual = signal.copy()
        selected_atoms = []
        code = np.zeros(self.n_components)
        
        for _ in range(min(self.sparsity_constraint, self.n_components)):
            # Find atom with highest correlation
            correlations = np.abs(self.dictionary_.T @ residual)
            correlations[selected_atoms] = 0  # Zero out already selected atoms
            
            best_atom = np.argmax(correlations)
            
            if correlations[best_atom] < 1e-10:
                break
                
            selected_atoms.append(best_atom)
            
            # Solve least squares with selected atoms
            selected_dict = self.dictionary_[:, selected_atoms]
            coeffs = np.linalg.lstsq(selected_dict, signal, rcond=None)[0]
            
            # Update code and residual
            code[selected_atoms] = coeffs
            residual = signal - selected_dict @ coeffs
            
        return code
    
    def _initialize_dictionary(self, n_features: int, n_atoms: int) -> np.ndarray:
        """Initialize dictionary with normalized random atoms"""
        dictionary = np.random.randn(n_features, n_atoms).astype(np.float32)
        for i in range(n_atoms):
            dictionary[:, i] /= np.linalg.norm(dictionary[:, i])
        return dictionary
    
    def _reconstruction_error(self, X: np.ndarray, codes: np.ndarray) -> float:
        """Compute reconstruction error"""
        reconstruction = self.dictionary_ @ codes
        return np.mean(np.sum((X - reconstruction)**2, axis=0))


# =============================================================================
# Feature Extraction Components
# =============================================================================

class SparseFeatureExtractor:
    """
    Feature extraction using learned sparse dictionaries
    
    Provides high-level interface for extracting sparse features
    from images and other data types.
    """
    
    def __init__(self, 
                 sparse_coder: Optional[SparseCoder] = None,
                 patch_size: Tuple[int, int] = (8, 8),
                 overlap: float = 0.5):
        """
        Parameters
        ----------
        sparse_coder : SparseCoder, optional
            Pre-trained sparse coder. If None, will create default one.
        overlap : float
            Overlap between patches during feature extraction
        """
        self.sparse_coder = sparse_coder or SparseCoder()
        self.patch_size = patch_size
        self.overlap = overlap
        
        print(f"üé® SparseFeatureExtractor: {patch_size} patches, {overlap:.1%} overlap")
    
    def extract_patches_from_image(self, image: np.ndarray, 
                                 normalize: bool = True) -> np.ndarray:
        """
        Extract overlapping patches from image
        
        Parameters
        ----------
        image : array-like, shape (height, width) or (height, width, channels)
            Input image
        normalize : bool
            Whether to normalize patches (subtract mean)
            
        Returns
        -------
        patches : array, shape (n_patches, patch_height * patch_width [* channels])
            Extracted patches
        """
        if image.ndim == 3:
            # Convert to grayscale if color
            image = np.mean(image, axis=2)
            
        h, w = image.shape
        ph, pw = self.patch_size
        
        # Calculate step size based on overlap
        step_h = max(1, int(ph * (1 - self.overlap)))
        step_w = max(1, int(pw * (1 - self.overlap)))
        
        patches = []
        
        for y in range(0, h - ph + 1, step_h):
            for x in range(0, w - pw + 1, step_w):
                patch = image[y:y+ph, x:x+pw].flatten()
                
                if normalize:
                    patch = patch - np.mean(patch)
                    std = np.std(patch)
                    if std > 1e-8:
                        patch = patch / std
                        
                patches.append(patch)
        
        return np.array(patches)
    
    def fit_on_images(self, images: List[np.ndarray]) -> 'SparseFeatureExtractor':
        """
        Train sparse coder on a collection of images
        
        Parameters
        ---------- 
        images : list of arrays
            Training images
            
        Returns
        -------
        self : SparseFeatureExtractor
        """
        print(f"üéì Training on {len(images)} images...")
        
        # Extract patches from all images
        all_patches = []
        for i, image in enumerate(images):
            patches = self.extract_patches_from_image(image)
            all_patches.append(patches)
            
            if i % 10 == 0:
                print(f"   Processed {i+1}/{len(images)} images")
        
        # Combine all patches
        all_patches = np.vstack(all_patches)
        print(f"   Total patches: {len(all_patches)}")
        
        # Train sparse coder
        self.sparse_coder.fit(all_patches)
        
        return self
    
    def extract_features(self, image: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Extract sparse features from image
        
        Returns
        -------
        features : dict
            Dictionary containing:
            - 'codes': sparse coefficients
            - 'patches': original patches
            - 'reconstructed': reconstructed patches
        """
        patches = self.extract_patches_from_image(image)
        codes = self.sparse_coder.transform(patches)
        reconstructed = self.sparse_coder.reconstruct(patches)
        
        return {
            'codes': codes,
            'patches': patches,
            'reconstructed': reconstructed,
            'sparsity': np.mean(np.sum(codes != 0, axis=1)),
            'reconstruction_error': np.mean(np.sum((patches - reconstructed)**2, axis=1))
        }


# =============================================================================
# Batch Processing Components
# =============================================================================

class BatchProcessor:
    """
    Efficient batch processing for large-scale sparse coding
    
    Handles memory management and parallel processing for
    large datasets that don't fit in memory.
    """
    
    def __init__(self, 
                 sparse_coder: SparseCoder,
                 batch_size: int = 1000,
                 n_jobs: int = 1):
        """
        Parameters
        ----------
        batch_size : int
            Number of samples per batch
        n_jobs : int
            Number of parallel jobs (currently not implemented)
        """
        self.sparse_coder = sparse_coder
        self.batch_size = batch_size
        self.n_jobs = n_jobs
        
        print(f"‚ö° BatchProcessor: {batch_size} samples per batch")
    
    def process_large_dataset(self, data_generator, total_samples: Optional[int] = None):
        """
        Process large dataset that doesn't fit in memory
        
        Parameters
        ----------
        data_generator : generator
            Generator that yields batches of data
        total_samples : int, optional
            Total number of samples (for progress tracking)
        """
        results = {
            'codes': [],
            'reconstruction_errors': [],
            'sparsity_levels': []
        }
        
        processed = 0
        
        for batch_data in data_generator:
            # Transform batch
            batch_codes = self.sparse_coder.transform(batch_data)
            batch_reconstructed = self.sparse_coder.reconstruct(batch_data)
            
            # Compute metrics
            reconstruction_error = np.mean(np.sum((batch_data - batch_reconstructed)**2, axis=1))
            sparsity = np.mean(np.sum(batch_codes != 0, axis=1))
            
            # Store results
            results['codes'].append(batch_codes)
            results['reconstruction_errors'].append(reconstruction_error)
            results['sparsity_levels'].append(sparsity)
            
            processed += len(batch_data)
            
            if total_samples:
                progress = processed / total_samples * 100
                print(f"   Processed {processed:,}/{total_samples:,} ({progress:.1f}%)")
            else:
                print(f"   Processed {processed:,} samples")
        
        return results


# =============================================================================
# Utility Functions
# =============================================================================

def create_overcomplete_basis(n_features: int, n_components: int, 
                            basis_type: str = 'random') -> np.ndarray:
    """
    Create overcomplete basis for initialization
    
    Parameters
    ----------
    n_features : int
        Dimensionality of input data
    n_components : int  
        Number of basis functions (> n_features for overcomplete)
    basis_type : str
        Type of basis: 'random', 'dct', 'gabor', 'ica'
        
    Returns
    -------
    basis : array, shape (n_components, n_features)
        Overcomplete basis matrix
    """
    if basis_type == 'random':
        basis = np.random.randn(n_components, n_features)
    elif basis_type == 'ica':
        # Use ICA to initialize basis
        ica = FastICA(n_components=min(n_components, n_features), random_state=42)
        # Create some random data for ICA initialization
        X_dummy = np.random.randn(1000, n_features)
        ica.fit(X_dummy)
        
        basis = ica.components_
        
        # Add random components if overcomplete
        if n_components > n_features:
            extra_components = n_components - n_features
            extra_basis = np.random.randn(extra_components, n_features)
            basis = np.vstack([basis, extra_basis])
    else:
        # Default to random
        basis = np.random.randn(n_components, n_features)
    
    # Normalize each basis function
    for i in range(n_components):
        basis[i] /= np.linalg.norm(basis[i]) + 1e-8
    
    return basis


def lateral_inhibition(codes: np.ndarray, inhibition_strength: float = 0.1) -> np.ndarray:
    """
    Apply lateral inhibition to sparse codes
    
    Implements competitive dynamics where active neurons
    inhibit nearby neurons, promoting sparsity.
    
    Parameters
    ----------
    codes : array, shape (n_samples, n_components)
        Sparse coefficient matrix
    inhibition_strength : float
        Strength of lateral inhibition
        
    Returns
    -------
    inhibited_codes : array, shape (n_samples, n_components)
        Codes after lateral inhibition
    """
    inhibited_codes = codes.copy()
    
    for i in range(codes.shape[0]):
        # Find active neurons
        active = np.abs(codes[i]) > np.mean(np.abs(codes[i]))
        
        if np.sum(active) > 1:
            # Apply inhibition from most active neuron
            max_idx = np.argmax(np.abs(codes[i]))
            
            for j in range(codes.shape[1]):
                if j != max_idx and active[j]:
                    # Distance-based inhibition (closer neurons inhibited more)
                    distance = abs(j - max_idx)
                    inhibition = inhibition_strength * np.abs(codes[i, max_idx]) / (1 + distance)
                    
                    if codes[i, j] > 0:
                        inhibited_codes[i, j] = max(0, codes[i, j] - inhibition)
                    else:
                        inhibited_codes[i, j] = min(0, codes[i, j] + inhibition)
    
    return inhibited_codes


def process_large_dataset(dataset_path: str, sparse_coder: SparseCoder, 
                        batch_size: int = 1000) -> Dict[str, Any]:
    """
    Process large dataset from disk with memory management
    
    Parameters
    ----------
    dataset_path : str
        Path to dataset file
    sparse_coder : SparseCoder
        Trained sparse coder
    batch_size : int
        Processing batch size
        
    Returns
    -------
    results : dict
        Processing results and statistics
    """
    processor = BatchProcessor(sparse_coder, batch_size)
    
    # This would be implemented based on specific dataset format
    # For now, return placeholder
    return {
        'message': f'Would process dataset at {dataset_path} with batch size {batch_size}',
        'batch_processor': processor
    }


if __name__ == "__main__":
    # Example usage
    print("üéØ Sparse Coding Library - Core Implementation")
    print("=" * 50)
    
    # Create sample data
    np.random.seed(42)
    n_samples, n_features = 500, 64
    X = np.random.randn(n_samples, n_features)
    
    # Test unified SparseCoder
    print("\nüß™ Testing Unified SparseCoder:")
    coder = SparseCoder(n_components=50, max_iter=100, random_state=42)
    coder.fit(X)
    
    codes = coder.transform(X[:10])
    print(f"   Sparse codes shape: {codes.shape}")
    print(f"   Average sparsity: {np.mean(np.sum(codes != 0, axis=1)):.1f} non-zeros")
    
    # Test Original Olshausen-Field
    print("\nüî¨ Testing Original Olshausen-Field:")
    original = OlshausenFieldOriginal(n_components=25, patch_size=(8, 8))
    
    # Create fake image data
    fake_images = [np.random.randn(32, 32) for _ in range(5)]
    original.learn_dictionary(fake_images, n_iterations=100)
    
    print("\n‚úÖ All components working correctly!")